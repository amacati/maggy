{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hops import hdfs\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(module, hparams, train_set, test_set):\n",
    "    \n",
    "    import time\n",
    "    import os\n",
    "    import pickle\n",
    "    \n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.distributed as dist\n",
    "    from torch.utils.data import DataLoader\n",
    "    from torch.cuda.amp import GradScaler, autocast\n",
    "    from torchvision import transforms as T\n",
    "        \n",
    "    from hops import hdfs\n",
    "\n",
    "    \n",
    "    model = module(**hparams)\n",
    "    \n",
    "    n_epochs = 1\n",
    "    n_exec = 2\n",
    "    batch_size = 128\n",
    "    lr_base = 0.1 * n_exec*batch_size/256\n",
    "    \n",
    "    def train_transform(image_net_row):\n",
    "        transform = T.Compose([\n",
    "            T.ToTensor(),\n",
    "            T.RandomCrop(224),\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                        std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        return {\"image\": transform(image_net_row['image']), \"label\": image_net_row['label']}\n",
    "    \n",
    "    def test_transform(image_net_row):\n",
    "        transform = T.Compose([\n",
    "            T.ToTensor(),\n",
    "            T.CenterCrop(224),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                        std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        return {\"image\": transform(image_net_row['image']), \"label\": image_net_row['label']}\n",
    "\n",
    "    # Parameters as in https://arxiv.org/pdf/1706.02677.pdf\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr_base, momentum=0.9, weight_decay=0.0001, nesterov=True)\n",
    "\n",
    "    loss_criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    train_loader = DataLoader(train_set, pin_memory=True, batch_size=batch_size, transform_spec=train_transform)\n",
    "    test_loader = DataLoader(test_set, pin_memory=True, batch_size=batch_size, transform_spec=test_transform)\n",
    "                \n",
    "    def time_to_h_m_s(t_diff):\n",
    "        minutes, seconds = divmod(t_diff, 60)\n",
    "        hours, minutes = divmod(minutes, 60)\n",
    "        return hours, minutes, seconds\n",
    "\n",
    "    def print_train_time(t_0, batch, n_batches, epoch, n_epochs):\n",
    "        t_diff = time.time() - t_0\n",
    "        tr_time = time_to_h_m_s(t_diff)\n",
    "        t_est = t_diff * (n_epochs*n_batches/(epoch*n_batches + idx+1) - 1)\n",
    "        est_time = time_to_h_m_s(t_est)\n",
    "        print(\"Training time: {:.0f}h {:.0f}m {:.0f}s\\nEstimated remaining time: {:.0f}h {:.0f}m {:.0f}s.\".format(*tr_time, *est_time))\n",
    "    \n",
    "    ### Logging ###\n",
    "    log_path = hdfs.project_path() + \"Experiments/\" + config.name + \"/training_log_\" + os.environ[\"RANK\"] + \".log\"\n",
    "    if hdfs.exists(log_path) and hdfs.isfile(log_path):\n",
    "        hdfs.delete(log_path)\n",
    "    hdfs.dump(\"ep,lr,top1acc,t_load,t_forward,t_backward,t_step,t_ep,mem_f_g,mem_b_g,mem_op_g,checkpoint_time\", log_path)\n",
    "        \n",
    "    def log_training(ep, top1acc, t_load, t_forward, t_backward, t_step, t_ep, mem_f_g, mem_b_g, mem_op_g, checkpoint_time):\n",
    "        f = hdfs.open_file(log_path, flags=\"at\")\n",
    "        try:\n",
    "            f.write(\"\\n\")\n",
    "            f.write(f\"{ep},{lr_schedule(ep)},{top1acc},{t_load},{t_forward},{t_backward},{t_step},{t_ep},{mem_f_g},{mem_b_g},{mem_op_g},{checkpoint_time}\")\n",
    "        finally:\n",
    "            f.close()\n",
    "        \n",
    "    def eval_model(model, test_loader):\n",
    "        acc = 0\n",
    "        model.eval()\n",
    "        img_cnt = 0\n",
    "        with torch.no_grad():\n",
    "            with model.join():\n",
    "                for idx, data in enumerate(test_loader):\n",
    "                    print(\"Testing batch {}\".format(idx))\n",
    "                    img, label = data[\"image\"].float(), data[\"label\"].float()  # permute(0,3,1,2).contiguous().\n",
    "                    prediction = model(img)\n",
    "                    acc += torch.sum(torch.argmax(prediction, dim=1) == label).detach()\n",
    "                    img_cnt += len(label.detach())\n",
    "        acc = acc/float(img_cnt)\n",
    "        print(\"Test accuracy: {:.3f}\".format(acc))\n",
    "        print(\"-\"*20)\n",
    "        return acc\n",
    "    \n",
    "    def checkpoint(model, optimizer, epoch):\n",
    "        print(\"Saving model...\")\n",
    "        checkpoint_ = {\"model\": model.state_dict(), \"optimizer\": optimizer.state_dict(), \"epoch\": epoch}\n",
    "        path = hdfs.project_path() + \"Experiments/checkpoint/\" + \"model_e\" + str(epoch)\n",
    "        try:\n",
    "            hdfs.dump(pickle.dumps(checkpoint_), path)\n",
    "            print(\"Model saved.\")\n",
    "        except:\n",
    "            print(\"Save abort.\")\n",
    "    \n",
    "    def lr_schedule(epoch):\n",
    "        if epoch in range(5):\n",
    "            return lr_base * (epoch+1)/5\n",
    "        elif epoch < 30:\n",
    "            return lr_base\n",
    "        elif epoch < 60:\n",
    "            return lr_base * 0.1\n",
    "        elif epoch < 80:\n",
    "            return lr_base * 0.1**2\n",
    "        return lr_base * 0.1**3\n",
    "\n",
    "    # Mixed precision training with learning rate scheduling.\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    model.train()\n",
    "    t_0 = time.time()\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"-\"*20 + \"\\nStarting new epoch\\n\")\n",
    "        model.train()\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = lr_schedule(epoch)\n",
    "        print(\"Epoch {}, lr: {}\".format(epoch, lr_schedule(epoch)))\n",
    "        t_load = 0\n",
    "        t_forward = 0\n",
    "        t_backward = 0\n",
    "        t_step = 0\n",
    "        t_ep_0 = time.time()\n",
    "\n",
    "        with model.join():\n",
    "            t_load_0 = time.time()\n",
    "\n",
    "            for idx, data in enumerate(train_loader):\n",
    "                t_load_1 = time.time()\n",
    "                print(\"DataLoader load time: {:.3f}s, Batch: {}\".format(t_load_1-t_load_0, idx))\n",
    "                print(\"Batch size: {}\".format(len(data[\"image\"])))\n",
    "                if len(data[\"image\"]) != batch_size:\n",
    "                    print(\"Batch size mismatch detected\")\n",
    "                    continue\n",
    "                if len(data[\"image\"]) != batch_size:\n",
    "                    print(\"Unreachable flag\")\n",
    "                img, label = data[\"image\"].float(), data[\"label\"].float()  # .permute(0,3,1,2).contiguous()\n",
    "                with autocast():\n",
    "                    if idx == 0:\n",
    "                        mem_f_pre = torch.cuda.max_memory_allocated(0)\n",
    "                    t_forward_0 = time.time()\n",
    "                    prediction = model(img)\n",
    "                    t_forward_1 = time.time()\n",
    "                    if idx == 0:\n",
    "                        mem_f_post = mem_b_pre = torch.cuda.max_memory_allocated(0)\n",
    "                    loss = loss_criterion(prediction, label.long())\n",
    "                t_backward_0 = time.time()\n",
    "                scaler.scale(loss).backward()\n",
    "                t_backward_1 = time.time()\n",
    "                if idx == 0:\n",
    "                    mem_b_post = mem_op_pre = torch.cuda.max_memory_allocated(0)\n",
    "                t_step_0 = time.time()\n",
    "                scaler.step(optimizer)\n",
    "                t_step_1 = time.time()\n",
    "                if idx == 0:\n",
    "                    mem_op_post = torch.cuda.max_memory_allocated(0)\n",
    "\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                if idx%(10) == 0:\n",
    "                    print(f\"Working on batch {idx}\")\n",
    "\n",
    "                t_load += t_load_1 - t_load_0\n",
    "                t_forward += t_forward_1 - t_forward_0\n",
    "                t_backward += t_backward_1 - t_backward_0\n",
    "                t_step += t_step_1 - t_step_0\n",
    "                mem_f_g = mem_f_post - mem_f_pre\n",
    "                mem_b_g = mem_b_post - mem_b_pre\n",
    "                mem_op_g = mem_op_post - mem_op_pre\n",
    "\n",
    "                t_load_0 = time.time()\n",
    "                print(\"Batch computation time: {:.3f}s, Batch: {}\".format(t_load_0-t_load_1, idx))\n",
    "\n",
    "        t_ep_1 = t_check_0 = time.time()\n",
    "        if os.environ[\"RANK\"] == \"0\" and epoch%10 == 0:\n",
    "            checkpoint(model, optimizer, epoch)\n",
    "        t_check_1 = time.time()\n",
    "        print(\"Epoch training took {:.0f}s.\\n\".format(t_ep_1-t_ep_0))\n",
    "        acc = eval_model(model, test_loader)\n",
    "        log_training(epoch, acc, t_load, t_forward, t_backward, t_step, t_ep_1-t_ep_0, mem_f_g, mem_b_g, mem_op_g, t_check_1-t_check_0)\n",
    "    t_1 = time.time()\n",
    "    minutes, seconds = divmod(t_1 - t_0, 60)\n",
    "    hours, minutes = divmod(minutes, 60)\n",
    "    print(\"-\"*20 + \"\\nTotal training time: {:.0f}h {:.0f}m {:.0f}s.\".format(hours, minutes, seconds))\n",
    "    return float(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = hdfs.project_path() + \"DataSets/ImageNet/PetastormImageNette/train\"\n",
    "test_ds = hdfs.project_path() + \"DataSets/ImageNet/PetastormImageNette/test\"\n",
    "print(hdfs.exists(train_ds), hdfs.exists(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maggy import experiment\n",
    "from maggy.experiment_config import TorchDistributedConfig\n",
    "\n",
    "config = TorchDistributedConfig(name='ImageNet_ddp_Z1/2', module=models.resnet50, hparams={\"pretrained\": False}, train_set=train_ds, test_set=test_ds, backend=\"ddp\", zero_lvl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = experiment.lagom(train_fn, config)"
   ]
  }
 ]
}
